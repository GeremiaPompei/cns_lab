{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_a7yuMXIWd-u"
   },
   "source": [
    "# LAB 3.1 - CNS (Sequential MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "joGh-vojSn6F",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cde17eff-a66b-49be-d7b5-56764f1b5bd4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘sequential_mnist’: File exists\n",
      "mkdir: cannot create directory ‘sequential_mnist/variables’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir sequential_mnist\n",
    "!mkdir sequential_mnist / variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4nmzrL05Wd-w"
   },
   "source": [
    "Import of libraries, fix of random seed and device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MdyyaFnSWd-w"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from typing import Callable\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "iEBkp47CWd-2"
   },
   "source": [
    "# Bonus track 2 & 4 - Sequential MNIST classification task & benchmarking RNN models on the sequential MNIST task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Fglyu_7xWd-2"
   },
   "source": [
    "Function able to download and get tensors related to MNIST data and labels of train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1vDLcLIWd-2"
   },
   "outputs": [],
   "source": [
    "def download_mnist() -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Function able to download MNIST dataset and return it.\n",
    "\n",
    "    returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Training data and labels and test data and labels of MNIST dataset.\n",
    "    \"\"\"\n",
    "    mnist_dir = 'MNIST/'\n",
    "    if not Path(mnist_dir).exists():\n",
    "        os.mkdir(mnist_dir)\n",
    "    TR_MNIST = datasets.MNIST(root=f'{mnist_dir}', train=True, download=True, transform=None)\n",
    "    TS_MNIST = datasets.MNIST(root=f'{mnist_dir}', train=False, download=True, transform=None)\n",
    "    TR_DATA_MNIST = TR_MNIST.train_data.reshape(28 * 28, -1, 1).type(torch.float32).to(device)\n",
    "    TS_DATA_MNIST = TS_MNIST.test_data.reshape(28 * 28, -1, 1).type(torch.float32).to(device)\n",
    "    TR_LABELS_MNIST = torch.nn.functional.one_hot(TR_MNIST.train_labels).type(torch.float32).to(device)\n",
    "    TS_LABELS_MNIST = torch.nn.functional.one_hot(TS_MNIST.test_labels).type(torch.float32).to(device)\n",
    "    return TR_DATA_MNIST, TR_LABELS_MNIST, TS_DATA_MNIST, TS_LABELS_MNIST\n",
    "\n",
    "\n",
    "TR_DATA_MNIST, TR_LABELS_MNIST, TS_DATA_MNIST, TS_LABELS_MNIST = download_mnist()\n",
    "\n",
    "TR_DATA_MNIST.shape, TR_LABELS_MNIST.shape, TS_DATA_MNIST.shape, TS_LABELS_MNIST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function able to compute the accuracy metric."
   ],
   "metadata": {
    "id": "clhIQnwORbpt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def accuracy(out: torch.Tensor, pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Function that compute accuracy given an output and prediction tensor.\n",
    "\n",
    "    out: Output tensor.\n",
    "    pred: Prediction tensor.\n",
    "\n",
    "    returns:\n",
    "        float: Computed accuracy value.\n",
    "    \"\"\"\n",
    "    return (sum(pred.argmax(-1) - out.argmax(-1) == 0) / len(out)).item()"
   ],
   "metadata": {
    "id": "_W3eTYIYw15e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "mnp3e8TQWd-y"
   },
   "source": [
    "Train function able to fit a model given in input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SySCkfr6Wd-y"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 10,\n",
    "        batch_size: int = 64,\n",
    "        sgd_config: dict = {},\n",
    "        tqdm=None,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function able to train a given model.\n",
    "\n",
    "    model: Model to train.\n",
    "    TR: Tuple composed by X train and Y train torch tensors.\n",
    "    TS: Tuple composed by X test and Y test torch tensors.\n",
    "    epochs: Number of epochs of training.\n",
    "    batch_size: Dimension of batch.\n",
    "    sgd_config: Dictionary containing sgd configurations (lr and momentum).\n",
    "    tqdm: TQDM object to show the progressbar. It is None when progressbar is not shown.\n",
    "\n",
    "    returns:\n",
    "        tuple: Results of training. In particular the tuple is composed by 2 variables:\n",
    "            - train_accuracy: List of accuracy of training set computed for each epoch.\n",
    "            - test_accuracy: List of accuracy of test set computed for each epoch.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.SGD(model.parameters(), **sgd_config)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    train_accuracy, test_accuracy = None, None\n",
    "    X_TR, Y_TR = TR\n",
    "    X_TS, Y_TS = TS\n",
    "    model.eval()\n",
    "\n",
    "    iterable = range(epochs)\n",
    "    if tqdm is not None:\n",
    "        iterable = tqdm(iterable)\n",
    "    for _ in iterable:\n",
    "\n",
    "        model.train()\n",
    "        train_batch_accuracy = 0\n",
    "        for i in range(int(X_TR.shape[1] / batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            pred_tr = model(X_TR[:, i * batch_size: (i + 1) * batch_size])\n",
    "            TR_LABEL_BATCH = Y_TR[i * batch_size: (i + 1) * batch_size]\n",
    "            loss_tr = criterion(pred_tr, TR_LABEL_BATCH)\n",
    "            loss_tr.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            train_batch_accuracy += accuracy(pred_tr, TR_LABEL_BATCH)\n",
    "        train_accuracy = train_batch_accuracy / batch_size\n",
    "\n",
    "        model.eval()\n",
    "        test_batch_accuracy = 0\n",
    "        for i in range(int(X_TS.shape[1] / batch_size)):\n",
    "            pred_vl = model(X_TS[:, i * batch_size: (i + 1) * batch_size])\n",
    "            TS_LABEL_BATCH = Y_TS[i * batch_size: (i + 1) * batch_size]\n",
    "            test_batch_accuracy += accuracy(pred_vl, TS_LABEL_BATCH)\n",
    "        test_accuracy = test_batch_accuracy / batch_size\n",
    "\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4Ypfy6-3Wd-z"
   },
   "source": [
    "Gridsearch function able to find the best configuration for a model created in a `train_func` function callback, train the model with the best configuration and test it on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBZgnkTYWd-z"
   },
   "outputs": [],
   "source": [
    "def gridsearch(\n",
    "        train_func: Callable,\n",
    "        configs: dict,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 100,\n",
    "        vl_portion: float = 0.2,\n",
    "        batch_size: int = 64,\n",
    "        attempts_for_config: int = 1,\n",
    "        Ng: int = 1,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Gridsearch function able to find the best hyperparameters configuration, train the model with the best config and test it.\n",
    "\n",
    "    train_func: Function able to create a model and train it given a config, a train and validation set and a number of epochs.\n",
    "    configs: Hyperparameters configurations to investigate to find the best one that minimizes the loss on validation set. In particular this is a dictionary of lists for each hyperparam to investigate that is transformed by this function in a list of dictionaries.\n",
    "    TR: Training set data (X, Y).\n",
    "    TS: test set data (X, Y).\n",
    "    epochs: Number of epochs of training both for model selection and model evaluation.\n",
    "    vl_portion: Portion of example to use in validation set of model selection phase. It is useful to split training set in training and validation set.\n",
    "    attempts_for_config: Number of attempts to do for each configuration. The loss that it's minimized is the mean of each loss of each attempt.\n",
    "    Ng: Number of attempts in model assessment.\n",
    "\n",
    "    returns: A tuple of 4 variables related to the result of training function during the model evaluation phase (mean and std of training and ts accuracy).\n",
    "    \"\"\"\n",
    "    if isinstance(configs, dict):\n",
    "        configs = [dict(zip(configs.keys(), t)) for t in itertools.product(*configs.values())]\n",
    "    best_config = {}\n",
    "    best_accuracy = None\n",
    "    X_TR, Y_TR = TR\n",
    "    vl_size = int(X_TR.shape[1] * vl_portion)\n",
    "    for i, config in enumerate(tqdm(configs, desc='model evaluation')):\n",
    "        vl_accuracy = 0\n",
    "        for j in range(attempts_for_config):\n",
    "            _, eval_accuracy = train_func(\n",
    "                config,\n",
    "                (X_TR[:, :-vl_size], Y_TR[:-vl_size]),\n",
    "                (X_TR[:, -vl_size:], Y_TR[-vl_size:]),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "            vl_accuracy += eval_accuracy\n",
    "        vl_accuracy /= attempts_for_config\n",
    "        print(f'{i + 1}/{len(configs)} - Tried config {config} with accuracy {vl_accuracy}')\n",
    "        if best_accuracy is None or vl_accuracy > best_accuracy:\n",
    "            best_config = config\n",
    "            best_accuracy = vl_accuracy\n",
    "    print(f'Best config: {best_config} with accuracy {best_accuracy}')\n",
    "\n",
    "    print('Retraining...')\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    for i in tqdm(range(Ng), desc='model assessment'):\n",
    "        tr_accuracy, ts_accuracy = train_func(\n",
    "            best_config,\n",
    "            TR,\n",
    "            TS,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        train_accuracies.append(tr_accuracy)\n",
    "        test_accuracies.append(ts_accuracy)\n",
    "    train_accuracy_mean = np.mean(train_accuracies)\n",
    "    train_accuracy_std = np.std(train_accuracies)\n",
    "    test_accuracy_mean = np.mean(test_accuracies)\n",
    "    test_accuracy_std = np.std(test_accuracies)\n",
    "\n",
    "    return train_accuracy_mean, train_accuracy_std, test_accuracy_mean, test_accuracy_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "j6Z6jF6QWd-1"
   },
   "source": [
    "### RNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Antisymmetric rnn layer built as a torch module used to construct an antisymmetric recurrent neural network."
   ],
   "metadata": {
    "id": "RxvoLnN-TSLQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWX9iTKZiqtt"
   },
   "outputs": [],
   "source": [
    "class AntisymmetricRNNLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Antisymmetric rnn layer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            diffusion_coef: float = 0.001,\n",
    "            num_layers: int = 1,\n",
    "            bidirectional: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Antisymmetric rnn layer constructor.\n",
    "\n",
    "        input_size: Input size.\n",
    "        hidden_size: Hidden size.\n",
    "        diffusion_coef: Diffusion coefficient of antisymmetric layer.\n",
    "        num_layers: Number of layers to have a deep version of the model.\n",
    "        bidirectional: Flag to create the bidirectional version of the model.\n",
    "        \"\"\"\n",
    "        super(AntisymmetricRNNLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.D = 2 if self.bidirectional else 1\n",
    "        self.diffusion = diffusion_coef * torch.eye(hidden_size).to(device)\n",
    "        self.weight_in, self.weight_hh, self.bias = self.__init_weights()\n",
    "\n",
    "    def __init_weights(self) -> tuple[list, list, list]:\n",
    "        \"\"\"\n",
    "        Private method able to initialize model layers.\n",
    "\n",
    "        returns:\n",
    "          tuple[list, list, list]: Lists of layers:\n",
    "            - weight_in: Input weights.\n",
    "            - weight_hh: Hidden weights.\n",
    "            - bias: Bias.\n",
    "        \"\"\"\n",
    "        weight_in = [\n",
    "            torch.Tensor(self.input_size if i < self.D else self.hidden_size, self.hidden_size).to(device)\n",
    "            for i in range(self.num_layers * self.D)\n",
    "        ]\n",
    "        weight_hh = [\n",
    "            torch.Tensor(self.hidden_size, self.hidden_size).to(device)\n",
    "            for _ in range(self.num_layers * self.D)\n",
    "        ]\n",
    "        bias = [\n",
    "            torch.Tensor(1, self.hidden_size).to(device)\n",
    "            for _ in range(self.num_layers * self.D)\n",
    "        ]\n",
    "        return weight_in, weight_hh, bias\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            ts: torch.Tensor,\n",
    "            H: torch.Tensor = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward function used to the forward phase of pytorch module.\n",
    "\n",
    "        ts: Time series input data.\n",
    "        H: Previous hidden state.\n",
    "\n",
    "        returns:\n",
    "          tuple[torch.Tensor, torch.Tensor]: Output data:\n",
    "            - output: Output states of last layer.\n",
    "            - hidden: Hidden states of last time steps of each layer.\n",
    "        \"\"\"\n",
    "        layer_states = None\n",
    "        if H is None:\n",
    "            H = torch.rand(self.num_layers * self.D, ts.shape[1], self.hidden_size).to(device)\n",
    "        if self.bidirectional:\n",
    "            ts = torch.cat((ts, ts), dim=-1)\n",
    "        for l in range(0, self.num_layers, self.D):\n",
    "            dim_split = int(ts.shape[-1] / self.D)\n",
    "            layer_states = self.__forward_layer(ts[:, :, :dim_split], H, l)\n",
    "            if self.bidirectional:\n",
    "                layer_states_2 = self.__forward_layer(ts[:, :, dim_split:].flip(0), H, l + 1)\n",
    "                layer_states = torch.cat((layer_states, layer_states_2.flip(0)), dim=-1)\n",
    "            ts = layer_states\n",
    "        return layer_states, H\n",
    "\n",
    "    def __forward_layer(\n",
    "            self,\n",
    "            ts: torch.Tensor,\n",
    "            H: torch.Tensor,\n",
    "            l: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Private method able to run. the forward for a single layer.\n",
    "\n",
    "        ts: Time series in input.\n",
    "        H: hidden states.\n",
    "        l: number of layer.\n",
    "\n",
    "        returns:\n",
    "          torch.Tensor: Hidden states computed.\n",
    "        \"\"\"\n",
    "        layer_states = []\n",
    "        for x in ts:\n",
    "            H[l] = torch.nn.functional.tanh(\n",
    "                x @ self.weight_in[l] + H[l] @ (self.weight_hh[l] - self.weight_hh[l].T - self.diffusion) + self.bias[l]\n",
    "            )\n",
    "        layer_states.append(H[l])\n",
    "        return torch.stack(layer_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN pytorch model. This is able to take in input the parameter `recurrent_layer` that defines the type of recurrent layer. In this notebook are used:\n",
    "- torch.nn.RNN\n",
    "- torch.nn.LSTM\n",
    "- torch.nn.GRU\n",
    "- AntisymmetricRNNLayer (custom layer)"
   ],
   "metadata": {
    "id": "rjmrRQvJTRHV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FC5Esj3LWd-1"
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class of RNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            output_size: int,\n",
    "            recurrent_layer: torch.nn.RNN = torch.nn.RNN,\n",
    "            stateful: bool = True,\n",
    "            n_layers: int = 1,\n",
    "            bidirectional=False,\n",
    "            device: str = 'cpu',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        RNN constructor method.\n",
    "\n",
    "        input_size: Size of input value.\n",
    "        hidden_size: Size of hidden state.\n",
    "        output_size: Size of output value.\n",
    "        stateful: Boolean set to true if it's want to use the final training hidden state as initial hidden state of evaluation.\n",
    "        n_layers: Number of hidden layers. Default this is 1.\n",
    "        device: Name of device to use for computation.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.recoursive_layer = recurrent_layer(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        ).to(device)\n",
    "        D = 2 if bidirectional else 1\n",
    "        self.output_layer = torch.nn.Linear(hidden_size * D, output_size).to(device)\n",
    "        self.stateful = stateful\n",
    "        self.recurrent_states = None\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward function used to the forward phase of pytorch module.\n",
    "\n",
    "        X: Input data.\n",
    "\n",
    "        returns:\n",
    "            torch.Tensor: Output data.\n",
    "        \"\"\"\n",
    "        out_state, recurrent_states = self.recoursive_layer(X, self.recurrent_states)\n",
    "        if self.stateful and self.training:\n",
    "            if type(recurrent_states) == tuple:\n",
    "                self.recurrent_states = (\n",
    "                    recurrent_states[0].detach(),\n",
    "                    recurrent_states[1].detach()\n",
    "                )\n",
    "            else:\n",
    "                self.recurrent_states = recurrent_states.detach()\n",
    "        return self.output_layer(out_state[-1])\n",
    "\n",
    "\n",
    "RNN(1, 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QUYnDjg5Wd-1"
   },
   "source": [
    "RNN train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC0MXQ6pWd-1"
   },
   "outputs": [],
   "source": [
    "def train_rnn(\n",
    "        config: dict,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 10,\n",
    "        batch_size: int = 64,\n",
    "        tqdm=None,\n",
    "        device: str = 'cpu',\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function used to train the RNN model. It wraps the general train function.\n",
    "\n",
    "    config: Dictionary of hyperparameters.\n",
    "    TR: Training set.\n",
    "    TS: Test set.\n",
    "    epochs: Number of epochs.\n",
    "    batch_size: Dimension of a batch.\n",
    "    tqdm: Object used to show the progressbar.\n",
    "    device: Name of device to use for computation.\n",
    "\n",
    "    returns:\n",
    "        tuple: Train results.\n",
    "    \"\"\"\n",
    "    model = RNN(TR[0].shape[-1], config['hidden_size'], TR[1].shape[-1], recurrent_layer=config['recurrent_layer'],\n",
    "                n_layers=config['n_layers'], bidirectional=config['bidirectional'], device=device)\n",
    "    return train(model, TR, TS, epochs=epochs, tqdm=tqdm, device=device, batch_size=batch_size, sgd_config={\n",
    "        'lr': config['lr'],\n",
    "        'momentum': config['momentum'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MjjiBL1eWd-2"
   },
   "source": [
    "Function able to perform RNN gridsearch and plot of results related to MSE loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LGYfmTRWd-3"
   },
   "outputs": [],
   "source": [
    "def perform_rnn_gs_and_plot(\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        recurrent_layer: torch.nn.Module,\n",
    "        save_name: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Function able to perform RNN gridsearch and plot of results related to MSE loss and accuracy.\n",
    "\n",
    "    TR: Training set.\n",
    "    TS: Test set.\n",
    "    recurrent_layer: Recurrent layer module.\n",
    "    save_name: Name given to the file where is saved the model.\n",
    "    \"\"\"\n",
    "    tr_acc_mean, tr_acc_std, ts_acc_mean, ts_acc_std = gridsearch(\n",
    "        train_func=train_rnn,\n",
    "        configs=dict(\n",
    "            hidden_size=[100],\n",
    "            n_layers=[2, 1],\n",
    "            lr=[0.1],\n",
    "            momentum=[0.9],\n",
    "            bidirectional=[True, False],\n",
    "            recurrent_layer=[recurrent_layer],\n",
    "        ),\n",
    "        batch_size=512,\n",
    "        TR=TR,\n",
    "        TS=TS,\n",
    "        epochs=30,\n",
    "        vl_portion=0.2,\n",
    "        attempts_for_config=1,\n",
    "        Ng=5,\n",
    "    )\n",
    "    accuracy_results = dict(\n",
    "        training_accuracy_mean=tr_acc_mean,\n",
    "        training_accuracy_std=tr_acc_std,\n",
    "        test_accuracy_mean=ts_acc_mean,\n",
    "        test_accuracy_std=ts_acc_std,\n",
    "    )\n",
    "    print('Accuracy results')\n",
    "    print(json.dumps(accuracy_results, indent=2))\n",
    "    table_path = f'sequential_mnist/variables/accuracy_table.json'\n",
    "    if os.path.exists(table_path):\n",
    "        with open(table_path) as file:\n",
    "            accuracy_table = json.load(file)\n",
    "    else:\n",
    "        accuracy_table = {}\n",
    "    accuracy_table[save_name] = accuracy_results\n",
    "    with open(table_path, 'w') as file:\n",
    "        json.dump(accuracy_table, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "81GH4sQZWd-3"
   },
   "source": [
    "## Sequential MNIST model selection and model evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "seq_mnist_table_results = []"
   ],
   "metadata": {
    "id": "pRr7FSXPgw3c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vanilla RNN"
   ],
   "metadata": {
    "id": "B6VfkB8tXy6r"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcIl6E6XWd-3"
   },
   "outputs": [],
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_MNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_MNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.RNN,\n",
    "    save_name='rnn_seq_mnist'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM"
   ],
   "metadata": {
    "id": "GHc-1KtWX3E1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_MNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_MNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.LSTM,\n",
    "    save_name='lstm_seq_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "0t6eDZ53PYbY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GRU"
   ],
   "metadata": {
    "id": "VnQGFXSBX5NV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_MNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_MNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.GRU,\n",
    "    save_name='gru_seq_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "8BL6cv1JPYnD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Antisymmetric RNN"
   ],
   "metadata": {
    "id": "h6viJei7X7Mr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_MNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_MNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=AntisymmetricRNNLayer,\n",
    "    save_name='antisymmetric_rnn_seq_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "D6OjHnaBPYur"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Permuted sequential MNIST model selection and model evaluation results"
   ],
   "metadata": {
    "id": "J9L3hoXgYCB4"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bWf6uJMQWd-3"
   },
   "source": [
    "MNIST dataset permutation of data to perform permuted sequential MNSIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_WFRUEDWd-3"
   },
   "outputs": [],
   "source": [
    "permutations = np.random.permutation(28 * 28)\n",
    "\n",
    "TR_DATA_PMNIST = TR_DATA_MNIST[permutations]\n",
    "TS_DATA_PMNIST = TS_DATA_MNIST[permutations]\n",
    "\n",
    "TR_DATA_PMNIST.shape, TS_DATA_PMNIST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Py4p4g4VWd-3"
   },
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLFetZaTWd-3"
   },
   "outputs": [],
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_PMNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_PMNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.RNN,\n",
    "    save_name='rnn_permuted_mnist'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LSTM"
   ],
   "metadata": {
    "id": "xHkh0ozzYLKU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_PMNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_PMNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.LSTM,\n",
    "    save_name='lstm_permuted_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "g3GUbEvUQBmK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GRU"
   ],
   "metadata": {
    "id": "Tv652btRYOU_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_PMNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_PMNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=torch.nn.GRU,\n",
    "    save_name='gru_permuted_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "LNH1uJx9QBZx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Antisymmetric RNN"
   ],
   "metadata": {
    "id": "eL4IBp-3YQDH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_PMNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_PMNIST, TS_LABELS_MNIST),\n",
    "    recurrent_layer=AntisymmetricRNNLayer,\n",
    "    save_name='antisymmetric_rnn_permuted_mnist'\n",
    ")"
   ],
   "metadata": {
    "id": "d4UJqkgSQA_n"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
