{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LAB 3.1 - CNS (TDNN and RNN)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import of libraries, fix of random seed and device.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from typing import Callable\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading of dataset NARMA10."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([10000]), torch.Size([10000]))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'NARMA10.csv') as file:\n",
    "    data = file.read().split('\\n')[:-1]\n",
    "    X_narma10 = torch.Tensor([float(r) for r in data[0].split(',')])\n",
    "    Y_narma10 = torch.Tensor([float(r) for r in data[1].split(',')])\n",
    "\n",
    "X_narma10.shape, Y_narma10.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split in train and test set for each X and Y data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([5000]),\n torch.Size([5000]),\n torch.Size([5000]),\n torch.Size([5000]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_size = 5000\n",
    "\n",
    "TR_X_narma10 = X_narma10[:tr_size]\n",
    "TR_Y_narma10 = Y_narma10[:tr_size]\n",
    "TS_X_narma10 = X_narma10[tr_size:]\n",
    "TS_Y_narma10 = Y_narma10[tr_size:]\n",
    "\n",
    "TR_X_narma10.shape, TR_Y_narma10.shape, TS_X_narma10.shape, TS_Y_narma10.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions able to provide an easiest way to plot the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def _base_plot(\n",
    "        elements: list[tuple],\n",
    "        title: str = '',\n",
    "        xlabel: str = '',\n",
    "        ylabel: str = '',\n",
    "        xscale=None,\n",
    "        yscale=None,\n",
    "        save_name=None\n",
    ") -> None:\n",
    "    \"\"\"Function able to plot a list of elements.\"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title(title)\n",
    "    for line, label in elements:\n",
    "        plt.plot(line, label=label)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xscale is not None:\n",
    "        plt.xscale(xscale)\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    if save_name is not None:\n",
    "        plt.savefig(f'tdnn_and_rnn_plots/{save_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tr(\n",
    "        data: torch.Tensor,\n",
    "        prediction: torch.Tensor,\n",
    "        save_name: str = None\n",
    ") -> None:\n",
    "    \"\"\"Function able to plot the training y data and predictions to make a comparison between them.\"\"\"\n",
    "    if save_name is not None:\n",
    "        save_name = f'{save_name}_train'\n",
    "    prediction = prediction.reshape(-1).tolist()\n",
    "    _base_plot(\n",
    "        [\n",
    "            (data[-len(prediction):], 'time series'),\n",
    "            (prediction, 'prediction'),\n",
    "        ],\n",
    "        title='Time series of training set',\n",
    "        xlabel='X',\n",
    "        ylabel='Y',\n",
    "        save_name=save_name,\n",
    "        xscale='log',\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_ts(\n",
    "        data: torch.Tensor,\n",
    "        prediction: torch.Tensor,\n",
    "        save_name: str = None\n",
    ") -> None:\n",
    "    \"\"\"Function able to plot the test y data and predictions to make a comparison between them.\"\"\"\n",
    "    if save_name is not None:\n",
    "        save_name = f'{save_name}_test'\n",
    "    prediction = prediction.reshape(-1).tolist()\n",
    "    _base_plot(\n",
    "        [\n",
    "            (data[-len(prediction):], 'time series'),\n",
    "            (prediction, 'prediction'),\n",
    "        ],\n",
    "        title='Time series of test set',\n",
    "        xlabel='X',\n",
    "        ylabel='Y',\n",
    "        save_name=save_name,\n",
    "        xscale='log',\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_loss(\n",
    "        tr_loss: list,\n",
    "        ts_loss: list,\n",
    "        save_name: str = None\n",
    ") -> None:\n",
    "    \"\"\"Function able to plot the learning curve of training and test losses.\"\"\"\n",
    "    if save_name is not None:\n",
    "        save_name = f'{save_name}_loss'\n",
    "    _base_plot(\n",
    "        [\n",
    "            (tr_loss, 'TR loss'),\n",
    "            (ts_loss, 'TS loss'),\n",
    "        ],\n",
    "        title='Loss (MSE) for epochs',\n",
    "        xlabel='Epochs',\n",
    "        ylabel='Loss (MSE)',\n",
    "        save_name=save_name,\n",
    "        yscale='log',\n",
    "    )\n",
    "\n",
    "\n",
    "def general_plot(\n",
    "        train_loss: list,\n",
    "        ts_loss: list,\n",
    "        TR_Y: torch.Tensor,\n",
    "        train_pred: torch.Tensor,\n",
    "        TS_Y: torch.Tensor,\n",
    "        eval_pred: torch.Tensor,\n",
    "        save_name: str = None\n",
    ") -> None:\n",
    "    \"\"\"Function able to plot all results of subplots related to learning curve, training data and test data.\"\"\"\n",
    "    plot_loss(train_loss, ts_loss, save_name=save_name)\n",
    "    plot_tr(data=TR_Y, prediction=train_pred[-1], save_name=save_name)\n",
    "    plot_ts(data=TS_Y, prediction=eval_pred[-1], save_name=save_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train function able to fit a model given in input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: torch.nn.Module,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 10,\n",
    "        sgd_config: dict = {},\n",
    "        tqdm=None,\n",
    "        save_name: str = None,\n",
    "        device: str = 'cpu',\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function able to train a given model.\n",
    "\n",
    "    model: Model to train.\n",
    "    TR: Tuple composed by X train and Y train torch tensors.\n",
    "    TS: Tuple composed by X test and Y test torch tensors.\n",
    "    epochs: Number of epochs of training.\n",
    "    sgd_config: Dictionary containing sgd configurations (lr and momentum).\n",
    "    tqdm: TQDM object to show the progressbar. It is None when progressbar is not shown.\n",
    "    save_name: Name given to save the torch model and data in a file. If it is None the file is not created.\n",
    "    device: Name of device to use for computation.\n",
    "\n",
    "    returns:\n",
    "        tuple: Results of training. In particular the tuple is composed by 4 variables:\n",
    "            - train_loss: List of MSE loss of training set computed for each epoch.\n",
    "            - test_loss: List of MSE loss of test set computed for each epoch.\n",
    "            - train_preds: List of predictions of training set for each epoch.\n",
    "            - test_preds: List of predictions of test set for each epoch.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), **sgd_config).to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    train_loss, test_loss = [], []\n",
    "    train_preds, test_preds = [], []\n",
    "    X_TR, Y_TR = TR\n",
    "    X_TS, Y_TS = TS\n",
    "    model.eval()\n",
    "    test_loss.append(criterion(model(X_TS), Y_TS).item())\n",
    "\n",
    "    iterable = range(epochs)\n",
    "    if tqdm is not None:\n",
    "        iterable = tqdm(iterable)\n",
    "    for _ in iterable:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_tr = model(X_TR.to(device))\n",
    "        loss_tr = criterion(pred_tr, Y_TR.to(device))\n",
    "        loss_tr.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss_tr.item())\n",
    "        train_preds.append(pred_tr)\n",
    "\n",
    "        model.eval()\n",
    "        pred_vl = model(X_TS.to(device))\n",
    "        test_loss.append(criterion(pred_vl, Y_TS.to(device)).item())\n",
    "        test_preds.append(pred_vl)\n",
    "\n",
    "    train_loss.append(criterion(model(X_TR), Y_TR).item())\n",
    "    if save_name is not None:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss[-1],\n",
    "        }, f'tdnn_and_rnn_variables/{save_name}_model_and_tr_info.pt')\n",
    "    return train_loss, test_loss, train_preds, test_preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gridsearch function able to find the best configuration for a model created in a `train_func` function callback, train the model with the best configuration and test it on test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def gridsearch(\n",
    "        train_func: Callable,\n",
    "        configs: dict,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 100,\n",
    "        vl_tr_size: int = 4000,\n",
    "        attempts_for_config: int = 1,\n",
    "        save_name: str = None,\n",
    "        device: str = 'cpu',\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Gridsearch function able to find the best hyperparameters configuration, train the model with the best config and test it.\n",
    "\n",
    "    train_func: Function able to create a model and train it given a config, a train and validation set and a number of epochs.\n",
    "    configs: Hyperparameters configurations to investigate to find the best one that minimizes the loss on validation set. In particular this is a dictionary of lists for each hyperparam to investigate that is transformed by this function in a list of dictionaries.\n",
    "    TR: Training set data (X, Y).\n",
    "    TS: test set data (X, Y).\n",
    "    epochs: Number of epochs of training both for model selection and model evaluation.\n",
    "    vl_tr_size: Number of example to use in training set of model selection phase. It is useful to split training set in training and validation set.\n",
    "    attempts_for_config: Number of attempts to do for each configuration. The loss that it's minimized is the mean of each loss of each attempt.\n",
    "    save_name: Name used for saving the file related to mse info (mse of training, validation and test set).\n",
    "    device: Name of device to use for computation.\n",
    "\n",
    "    returns: A tuple of 4 variables related to the result of training function during the model evaluation phase (training on entire training set and test on test set).\n",
    "    \"\"\"\n",
    "    configs = [dict(zip(configs.keys(), t)) for t in itertools.product(*configs.values())]\n",
    "    best_config = {}\n",
    "    best_loss = None\n",
    "    X_TR, Y_TR = TR\n",
    "    for i, config in enumerate(tqdm(configs)):\n",
    "        vl_loss = 0\n",
    "        for j in range(attempts_for_config):\n",
    "            _, tdnn_eval_loss, _, _ = train_func(\n",
    "                config,\n",
    "                (X_TR[:vl_tr_size], Y_TR[:vl_tr_size]),\n",
    "                (X_TR[vl_tr_size:], Y_TR[vl_tr_size:]),\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "            )\n",
    "            vl_loss += tdnn_eval_loss[-1]\n",
    "        vl_loss /= attempts_for_config\n",
    "        print(f'{i + 1}/{len(configs)} - Tried config {config} with loss {vl_loss}')\n",
    "        if best_loss is None or vl_loss < best_loss:\n",
    "            best_config = config\n",
    "            best_loss = vl_loss\n",
    "    print(f'Best config: {best_config} with loss {best_loss}')\n",
    "    print('Retraining...')\n",
    "    train_loss, eval_loss, train_preds, eval_preds = train_func(\n",
    "        best_config,\n",
    "        TR,\n",
    "        TS,\n",
    "        epochs=epochs,\n",
    "        tqdm=tqdm,\n",
    "        save_name=save_name,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    if save_name is not None:\n",
    "        mse_data = {\n",
    "            'train_mse': train_loss[-1],\n",
    "            'validation_mse': best_loss,\n",
    "            'test_mse': eval_loss[-1]\n",
    "        }\n",
    "        print(mse_data)\n",
    "        with open(f'tdnn_and_rnn_variables/{save_name}_mse_data.json', 'w') as file:\n",
    "            json.dump(mse_data, file)\n",
    "\n",
    "    return train_loss, eval_loss, train_preds, eval_preds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TDNN\n",
    "\n",
    "TDNN pytorch model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "TDNN(\n  (hidden_layers): Sequential(\n    (0): Linear(in_features=11, out_features=100, bias=True)\n    (1): Linear(in_features=100, out_features=100, bias=True)\n  )\n  (output_layer): Linear(in_features=100, out_features=1, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TDNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class of TDNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window: int, hidden_size: int, output_size: int, n_layers: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        TDNN constructor method.\n",
    "\n",
    "        window: Window of TDNN that is related to the number of previous time steps seen in the past.\n",
    "        hidden_size: Size of hidden state.\n",
    "        output_size: Size of output value.\n",
    "        n_layers: Number of hidden layers. Default this is 1.\n",
    "        \"\"\"\n",
    "        super(TDNN, self).__init__()\n",
    "        hidden_layers = [torch.nn.Linear(window + 1, hidden_size)]\n",
    "        for _ in range(n_layers - 1):\n",
    "            hidden_layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "        self.hidden_layers = torch.nn.Sequential(*hidden_layers)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward function used to the forward phase of pytorch module.\n",
    "\n",
    "        X: Input data.\n",
    "\n",
    "        returns:\n",
    "            torch.Tensor: Output data.\n",
    "        \"\"\"\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            preactivation = hidden_layer(X)\n",
    "            X = torch.nn.functional.tanh(preactivation)\n",
    "        return self.output_layer(X)\n",
    "\n",
    "\n",
    "TDNN(10, 100, 1, n_layers=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TDNN train function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_tdnn(\n",
    "        config: dict,\n",
    "        TR: torch.Tensor,\n",
    "        TS: torch.Tensor,\n",
    "        epochs: int = 10,\n",
    "        tqdm=None,\n",
    "        save_name: str = None,\n",
    "        device: str = 'cpu',\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function used to train the TDNN model. It wraps the general train function.\n",
    "\n",
    "    config: Dictionary of hyperparameters.\n",
    "    TR: Training set.\n",
    "    TS: Test set.\n",
    "    epochs: Number of epochs.\n",
    "    tqdm: Object used to show the progressbar.\n",
    "    save_name: Able to provide the title of file containing pytorch model and training info.\n",
    "    device: Name of device to use for computation.\n",
    "\n",
    "    returns:\n",
    "        tuple: Train results.\n",
    "    \"\"\"\n",
    "    window = config['window']\n",
    "    model = TDNN(window, config['hidden_size'], 1, n_layers=config['n_layers'])\n",
    "    sets = []\n",
    "    for X, Y in [TR, TS]:\n",
    "        X = torch.unsqueeze(X.unfold(0, window + 1, 1), 0)\n",
    "        Y = Y[window:].reshape(1, -1, 1)\n",
    "        sets.append((X, Y))\n",
    "    return train(model, sets[0], sets[1], epochs=epochs, tqdm=tqdm, save_name=save_name, device=device, sgd_config={\n",
    "        'lr': config['lr'],\n",
    "        'momentum': config['momentum'],\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TDNN gridsearch and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b392994dea344c8a7afc451139902b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m tdnn_save_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtdnn_narma10\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 3\u001B[0m tdnn_train_loss, tdnn_ts_loss, tdnn_train_pred, tdnn_eval_pred \u001B[38;5;241m=\u001B[39m \u001B[43mgridsearch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_tdnn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfigs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mTR\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTR_X_narma10\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTR_Y_narma10\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mTS\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTS_X_narma10\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTS_Y_narma10\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvl_tr_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattempts_for_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtdnn_save_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m general_plot(tdnn_train_loss, tdnn_ts_loss, TR_Y_narma10, tdnn_train_pred, TS_Y_narma10, tdnn_eval_pred,\n\u001B[1;32m     22\u001B[0m              save_name\u001B[38;5;241m=\u001B[39mtdnn_save_name)\n",
      "Cell \u001B[0;32mIn[6], line 34\u001B[0m, in \u001B[0;36mgridsearch\u001B[0;34m(train_func, configs, TR, TS, epochs, vl_tr_size, attempts_for_config, save_name, device)\u001B[0m\n\u001B[1;32m     32\u001B[0m vl_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(attempts_for_config):\n\u001B[0;32m---> 34\u001B[0m     _, tdnn_eval_loss, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_TR\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43mvl_tr_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_TR\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43mvl_tr_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_TR\u001B[49m\u001B[43m[\u001B[49m\u001B[43mvl_tr_size\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_TR\u001B[49m\u001B[43m[\u001B[49m\u001B[43mvl_tr_size\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m     vl_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tdnn_eval_loss[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     42\u001B[0m vl_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m attempts_for_config\n",
      "Cell \u001B[0;32mIn[8], line 31\u001B[0m, in \u001B[0;36mtrain_tdnn\u001B[0;34m(config, TR, TS, epochs, tqdm, save_name, device)\u001B[0m\n\u001B[1;32m     29\u001B[0m     Y \u001B[38;5;241m=\u001B[39m Y[window:]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     30\u001B[0m     sets\u001B[38;5;241m.\u001B[39mappend((X, Y))\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtqdm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msave_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msgd_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmomentum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmomentum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 30\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, TR, TS, epochs, sgd_config, tqdm, save_name, device)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\n\u001B[1;32m      2\u001B[0m         model: torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule,\n\u001B[1;32m      3\u001B[0m         TR: \u001B[38;5;28mtuple\u001B[39m[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m         device: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     10\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[1;32m     11\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    Function able to train a given model.\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m            - test_preds: List of predictions of test set for each epoch.\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msgd_config)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     32\u001B[0m     criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss()\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-UniversityofPisa/UNIPI/2_anno_2022_2023/2_semestre/computational neuroscience/assignments/cns_lab/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:239\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[0;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    242\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "tdnn_save_name = 'tdnn_narma10'\n",
    "\n",
    "tdnn_train_loss, tdnn_ts_loss, tdnn_train_pred, tdnn_eval_pred = gridsearch(\n",
    "    train_func=train_tdnn,\n",
    "    configs=dict(\n",
    "        window=[8, 10, 12],\n",
    "        hidden_size=[100],\n",
    "        n_layers=[1, 2],\n",
    "        lr=[0.1, 0.01],\n",
    "        momentum=[0.5, 0.2],\n",
    "    ),\n",
    "    TR=(TR_X_narma10, TR_Y_narma10),\n",
    "    TS=(TS_X_narma10, TS_Y_narma10),\n",
    "    epochs=500,\n",
    "    vl_tr_size=4000,\n",
    "    attempts_for_config=5,\n",
    "    save_name=tdnn_save_name,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "general_plot(tdnn_train_loss, tdnn_ts_loss, TR_Y_narma10, tdnn_train_pred, TS_Y_narma10, tdnn_eval_pred,\n",
    "             save_name=tdnn_save_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RNN\n",
    "\n",
    "RNN pytorch model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class of RNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            hidden_size: int,\n",
    "            output_size: int,\n",
    "            stateful: bool = True,\n",
    "            n_layers: int = 1\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        RNN constructor method.\n",
    "\n",
    "        input_size: Size of input value.\n",
    "        hidden_size: Size of hidden state.\n",
    "        output_size: Size of output value.\n",
    "        stateful: Boolean set to true if it's want to use the final training hidden state as initial hidden state of evaluation.\n",
    "        n_layers: Number of hidden layers. Default this is 1.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.recoursive_layer = torch.nn.RNN(input_size, hidden_size, num_layers=n_layers)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.stateful = stateful\n",
    "        self.h = None\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward function used to the forward phase of pytorch module.\n",
    "\n",
    "        X: Input data.\n",
    "\n",
    "        returns:\n",
    "            torch.Tensor: Output data.\n",
    "        \"\"\"\n",
    "        state, h = self.recoursive_layer(X, self.h)\n",
    "        if self.stateful and self.training:\n",
    "            self.h = h.detach()\n",
    "        return self.output_layer(state)\n",
    "\n",
    "\n",
    "RNN(1, 100, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN train function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_rnn(\n",
    "        config: dict,\n",
    "        TR: tuple[torch.Tensor, torch.Tensor],\n",
    "        TS: tuple[torch.Tensor, torch.Tensor],\n",
    "        epochs: int = 10,\n",
    "        tqdm=None,\n",
    "        save_name: str = None,\n",
    "        device: str = 'cpu',\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Function used to train the RNN model. It wraps the general train function.\n",
    "\n",
    "    config: Dictionary of hyperparameters.\n",
    "    TR: Training set.\n",
    "    TS: Test set.\n",
    "    epochs: Number of epochs.\n",
    "    tqdm: Object used to show the progressbar.\n",
    "    save_name: Able to provide the title of file containing pytorch model and training info.\n",
    "    device: Name of device to use for computation.\n",
    "\n",
    "    returns:\n",
    "        tuple: Train results.\n",
    "    \"\"\"\n",
    "    model = RNN(TR[0].shape[-1], config['hidden_size'], TR[1].shape[-1], n_layers=config['n_layers'])\n",
    "    return train(model, TR, TS, epochs=epochs, tqdm=tqdm, save_name=save_name, device=device, sgd_config={\n",
    "        'lr': config['lr'],\n",
    "        'momentum': config['momentum'],\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RNN gridsearch and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn_save_name = 'rnn_narma10'\n",
    "\n",
    "rnn_train_loss, rnn_ts_loss, rnn_train_pred, rnn_eval_pred = gridsearch(\n",
    "    train_func=train_rnn,\n",
    "    configs=dict(\n",
    "        hidden_size=[100],\n",
    "        n_layers=[1, 2],\n",
    "        lr=[0.1, 0.01],\n",
    "        momentum=[0.5, 0.2],\n",
    "    ),\n",
    "    TR=(TR_X_narma10.reshape(-1, 1, 1), TR_Y_narma10.reshape(-1, 1, 1)),\n",
    "    TS=(TS_X_narma10.reshape(-1, 1, 1), TS_Y_narma10.reshape(-1, 1, 1)),\n",
    "    epochs=500,\n",
    "    vl_tr_size=4000,\n",
    "    attempts_for_config=3,\n",
    "    save_name=rnn_save_name,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "general_plot(rnn_train_loss, rnn_ts_loss, TR_Y_narma10, rnn_train_pred, TS_Y_narma10, rnn_eval_pred,\n",
    "             save_name=rnn_save_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus track 1 - Mackey-glass 17 task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mackey-glass 17 task dataset loading and split in train and test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tr_size = 5000\n",
    "\n",
    "with open(f'MG17.csv') as file:\n",
    "    data = file.read().split('\\n')[:-1][0]\n",
    "    data = torch.Tensor([float(r) for r in data.split(',')])\n",
    "    TR_mg17 = data[:tr_size]\n",
    "    TS_mg17 = data[tr_size:]\n",
    "\n",
    "TR_X_mg17 = TR_mg17[:-1]\n",
    "TR_Y_mg17 = TR_mg17[1:]\n",
    "TS_X_mg17 = TS_mg17[:-1]\n",
    "TS_Y_mg17 = TS_mg17[1:]\n",
    "\n",
    "TR_X_mg17.shape, TR_Y_mg17.shape, TS_X_mg17.shape, TS_Y_mg17.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TDNN\n",
    "\n",
    "TDNN gridsearch and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tdnn_save_name = 'tdnn_mg17'\n",
    "\n",
    "tdnn_train_loss, tdnn_ts_loss, tdnn_train_pred, tdnn_eval_pred = gridsearch(\n",
    "    train_func=train_tdnn,\n",
    "    configs=dict(\n",
    "        window=[8, 10, 12],\n",
    "        hidden_size=[100],\n",
    "        n_layers=[1, 2],\n",
    "        lr=[0.1, 0.01],\n",
    "        momentum=[0.5, 0.2],\n",
    "    ),\n",
    "    TR=(TR_X_mg17, TR_Y_mg17),\n",
    "    TS=(TS_X_mg17, TS_Y_mg17),\n",
    "    epochs=100,\n",
    "    vl_tr_size=4000,\n",
    "    attempts_for_config=5,\n",
    "    save_name=tdnn_save_name,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "general_plot(tdnn_train_loss, tdnn_ts_loss, TR_Y_mg17, tdnn_train_pred, TS_Y_mg17, tdnn_eval_pred,\n",
    "             save_name=tdnn_save_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RNN\n",
    "\n",
    "RNN gridsearch and results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rnn_save_name = 'rnn_mg17'\n",
    "\n",
    "rnn_train_loss, rnn_ts_loss, rnn_train_pred, rnn_eval_pred = gridsearch(\n",
    "    train_func=train_rnn,\n",
    "    configs=dict(\n",
    "        hidden_size=[100],\n",
    "        n_layers=[1, 2],\n",
    "        lr=[0.1, 0.01],\n",
    "        momentum=[0.5, 0.2],\n",
    "    ),\n",
    "    TR=(TR_X_mg17.reshape(-1, 1, 1), TR_Y_mg17.reshape(-1, 1, 1)),\n",
    "    TS=(TS_X_mg17.reshape(-1, 1, 1), TS_Y_mg17.reshape(-1, 1, 1)),\n",
    "    epochs=100,\n",
    "    vl_tr_size=4000,\n",
    "    attempts_for_config=3,\n",
    "    save_name=rnn_save_name,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "general_plot(rnn_train_loss, rnn_ts_loss, TR_Y_mg17, rnn_train_pred, TS_Y_mg17, rnn_eval_pred,\n",
    "             save_name=rnn_save_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus track 2 - Sequential MNIST classification task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function able to download and get tensors related to MNIST data and labels of train and test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_mnist() -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Function able to download MNIST dataset and return it.\n",
    "\n",
    "    returns:\n",
    "        tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Training data and labels and test data and labels of MNIST dataset.\n",
    "    \"\"\"\n",
    "    mnist_dir = 'MNIST/'\n",
    "    if not Path(mnist_dir).exists():\n",
    "        os.mkdir(mnist_dir)\n",
    "    TR_MNIST = datasets.MNIST(root=f'{mnist_dir}', train=True, download=True, transform=None)\n",
    "    TS_MNIST = datasets.MNIST(root=f'{mnist_dir}', train=False, download=True, transform=None)\n",
    "    TR_DATA_MNIST = TR_MNIST.train_data.reshape(-1, 28 * 28).type(torch.float32)\n",
    "    TS_DATA_MNIST = TS_MNIST.test_data.reshape(-1, 28 * 28).type(torch.float32)\n",
    "    TR_LABELS_MNIST = torch.nn.functional.one_hot(TR_MNIST.train_labels).type(torch.float32)\n",
    "    TS_LABELS_MNIST = torch.nn.functional.one_hot(TS_MNIST.test_labels).type(torch.float32)\n",
    "    return TR_DATA_MNIST, TR_LABELS_MNIST, TS_DATA_MNIST, TS_LABELS_MNIST\n",
    "\n",
    "\n",
    "TR_DATA_MNIST, TR_LABELS_MNIST, TS_DATA_MNIST, TS_LABELS_MNIST = download_mnist()\n",
    "\n",
    "TR_DATA_MNIST.shape, TR_LABELS_MNIST.shape, TS_DATA_MNIST.shape, TS_LABELS_MNIST.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Functions able to compute accuracy and plot a list of training and test accuracy for each epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy(out: torch.Tensor, pred: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Function that compute accuracy given an output and prediction tensor.\n",
    "\n",
    "    out: Output tensor.\n",
    "    pred: Prediction tensor.\n",
    "\n",
    "    returns:\n",
    "        float: Computed accuracy value.\n",
    "    \"\"\"\n",
    "    return (sum(pred.argmax(-1) - out.argmax(-1) == 0) / len(out)).item()\n",
    "\n",
    "\n",
    "def plot_accuracy(train_accuracy: list, test_accuracy: list, save_name: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Function able to plot the accuracy for each epoch.\n",
    "\n",
    "    train_accuracy: List of train accuracy for each epoch.\n",
    "    test_accuracy: List of test accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    if save_name is not None:\n",
    "        save_name = f'{save_name}_accuracy'\n",
    "    _base_plot(\n",
    "        [\n",
    "            (train_accuracy, 'TR accuracy'),\n",
    "            (test_accuracy, 'TS accuracy'),\n",
    "        ],\n",
    "        title='Accuracy for epochs',\n",
    "        xlabel='Epochs',\n",
    "        ylabel='Accuracy',\n",
    "        save_name=save_name,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function able to perform RNN gridsearch and plot of results related to MSE loss and accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def perform_rnn_gs_and_plot(TR: tuple[torch.Tensor, torch.Tensor], TS: tuple[torch.Tensor, torch.Tensor], save_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Function able to perform RNN gridsearch and plot of results related to MSE loss and accuracy.\n",
    "\n",
    "    TR: Training set.\n",
    "    TS: Test set.\n",
    "    save_name: Name given to the file where is saved the model.\n",
    "    \"\"\"\n",
    "    rnn_train_loss, rnn_ts_loss, rnn_train_pred, rnn_eval_pred = gridsearch(\n",
    "        train_func=train_rnn,\n",
    "        configs=dict(\n",
    "            hidden_size=[100],\n",
    "            n_layers=[1, 2],\n",
    "            lr=[0.1, 0.01],\n",
    "            momentum=[0.9, 0.2],\n",
    "        ),\n",
    "        TR=TR,\n",
    "        TS=TS,\n",
    "        epochs=50,\n",
    "        vl_tr_size=4000,\n",
    "        attempts_for_config=1,\n",
    "        save_name=save_name,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    plot_loss(rnn_train_loss, rnn_ts_loss, save_name=save_name)\n",
    "\n",
    "    tr_accuracy = [accuracy(TR[1], pred) for pred in rnn_train_pred]\n",
    "    ts_accuracy = [accuracy(TS[1], pred) for pred in rnn_eval_pred]\n",
    "    print(f'Accuracy: [TR: {tr_accuracy[-1]}] - [TS: {ts_accuracy[-1]}]')\n",
    "    plot_accuracy(tr_accuracy,ts_accuracy,save_name=save_name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sequential MNIST model selection and model evaluation results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_MNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_MNIST, TS_LABELS_MNIST),\n",
    "    save_name='rnn_seq_mnist'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MNIST dataset permutation od data to perform permuted sequential MNSIT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "permutations = np.random.permutation(28 * 28)\n",
    "\n",
    "TR_DATA_PMNIST = TR_DATA_MNIST[:, permutations]\n",
    "TS_DATA_PMNIST = TS_DATA_MNIST[:, permutations]\n",
    "\n",
    "TR_DATA_PMNIST.shape, TS_DATA_PMNIST.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Permuted MNIST model selection and model evaluation results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "perform_rnn_gs_and_plot(\n",
    "    (TR_DATA_PMNIST, TR_LABELS_MNIST),\n",
    "    (TS_DATA_PMNIST, TS_LABELS_MNIST),\n",
    "    save_name='rnn_permuted_mnist'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
